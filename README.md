<<<<<<< HEAD
# trt-llm-surgery
Automatic Precision Surgery for Long-Context LLM Deployment using TensorRT
=======

# TensorRT-LLM Surgery: Automatic Precision Surgery for Long-Context LLM Deployment

## ðŸŒŸ Why This Project Matters

### ðŸ”¥ High Relevance
Long-sequence inference (e.g., 32K+ tokens) has emerged as a key performance bottleneck in LLM deployment. Recent GTC sessions and research from NVIDIA highlight the growing industry focus on this challenge, including:
- [â€œAccelerate Super Longâ€‘Context LLM Inferenceâ€ (GTC 2025)](https://www.nvidia.com/en-us/on-demand/session/gtc25-s72568/)
- [â€œScaling to Millions of Tokens with Efficient Longâ€‘Context LLM Trainingâ€ (NVIDIA Developer Blog)](https://developer.nvidia.com/blog/scaling-to-millions-of-tokens-with-efficient-long-context-llm-training/)

### ðŸ§  Addressing Real Deployment Challenges
Handling dynamic batches, flexible input shapes, memory bottlenecks, and precision tradeoffs are common challenges in LLM inference. This project integrates those techniques into a practical workflow for optimizing long-sequence performance.

### ðŸŽ¯ Real-World Impact
This tool automatically identifies layer-level bottlenecks and applies precision surgery to balance latency, accuracy, and memory â€” simulating the kinds of tradeoffs made in production deployments.

---

## ðŸ§  Project Goals

Build a profiling tool and optimization pipeline that:

- Analyzes long-sequence LLM (e.g., Mistral-7B, LLaMA-3-8B) during TensorRT inference
- Automatically identifies attention-heavy or memory-bottleneck layers
- Applies selective precision overrides (FP16/INT8/FP32 mix)
- Outputs a customized TensorRT engine with improved latency + accuracy

---

## ðŸ“¦ Project Modules

```
trt-llm-surgery/
â”œâ”€â”€ profiler/
â”‚   â”œâ”€â”€ extract_layer_timings.py        # Parse TensorRT logs or use NVTX to get layer time
â”‚   â”œâ”€â”€ visualize_hot_layers.ipynb      # Visual heatmap of slowest layers per sequence length
â”‚
â”œâ”€â”€ optimizer/
â”‚   â””â”€â”€ precision_optimizer.py          # Algorithm to select best precision per layer
â”‚
â”œâ”€â”€ engine_builder/
â”‚   â””â”€â”€ build_trt_engine_custom_precision.py
â”‚
â”œâ”€â”€ runtime/
â”‚   â”œâ”€â”€ test_inference_latency.py       # Compare vanilla vs. optimized engine
â”‚   â””â”€â”€ memory_plot_generator.py
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ mistral7b_onnx/ â†’ (or llama3)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â””â”€â”€ before_after_latency_chart.png
â”‚
â””â”€â”€ README.md
```

---

## ðŸ’¥ Key Technical Highlights

| Capability                   | What It Demonstrates                            |
|------------------------------|-------------------------------------------------|
| Layer-level NVTX + profiler  | Identifies and debugs CUDA kernel bottlenecks   |
| Dynamic shape / batch        | Supports efficient inference under real inputs  |
| Precision fallback           | Maintains accuracy with quantized execution     |
| Engine customization         | Adapts engine behavior to deployment needs      |
| Memory profiling + charts    | Visualizes runtime bottlenecks and improvements |
>>>>>>> b2cfb7c (Initial commit with README)
